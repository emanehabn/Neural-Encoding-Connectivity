<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Dynamics Based Neural Encoding with Inter-Intra Region Connectivity.">
  <meta name="keywords" content="Neural Encoding,  Inter-Intra Region Connectivity">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Dynamics Based Neural Encoding with Inter-Intra Region Connectivity</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon3.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Dynamics Based Neural Encoding with Inter-Intra Region Connectivity</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/mai-gamal-el-katatny/">Mai Gamal</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/muhammed-abdel-hamid-8a5325178/">Mohamed Rashad</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/emanehabn/">Eman Ehab</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/seif-eldawlatly-0433901aa/">Seif Eldawlatly</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://msiam.github.io/homepage/">Mennatullah Siam</a><sup>5</sup>,
            </span>
            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>German University in Cairo, Egypt</span>
            <span class="author-block"><sup>2</sup>Ain Shams University, Egypt</span>
            <span class="author-block"><sup>3</sup>Nile University, Egypt</span>
            <span class="author-block"><sup>4</sup>American University in Cairo, Egypt</span>
            <span class="author-block"><sup>5</sup>University of British Columbia, Canada</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2402.12519"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.12519"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/MaiGamalEl-Katatny/dynamics_based_neural_encoding"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/fig1.png" alt="Paper Image" class="interpolation-image">
      <h2 class="subtitle has-text-centered">
        First large-scale study on how the brain processes video stimuli while considering full connectivity. It compares learned representations within deep video understanding models to fMRI recordings of the visual cortex regions.
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Extensive literature has drawn comparisons between recordings of biological neurons in the brain and deep neural networks. This comparative analysis aims to
            advance and interpret deep neural networks and enhance our understanding of
            biological neural systems. However, previous works did not consider the time
            aspect and how the encoding of video and dynamics in deep networks relate to
            the biological neural systems within a large-scale comparison. </p>
            
            <p>Towards this end, we propose the first large-scale study focused on comparing video understanding
            models with respect to the visual cortex recordings using video stimuli. The study
            encompasses more than two million regression fits, examining image vs. video
            understanding, convolutional vs. transformer-based and fully vs. self-supervised
            models. </p>
            
            <p>Our study resulted in both, insights to help better understand deep video
            understanding models and a novel neural encoding scheme to better encode biological neural systems. We provide key insights on how video understanding models
            predict visual cortex responses; showing video understanding better than image
            understanding models, convolutional models are better in the early-mid visual
            cortical regions than transformer based ones except for multiscale transformers and
            that two-stream models are better than single stream.  </p>

            <p>Furthermore, we propose a novel neural encoding scheme that is built on top of the best performing video
              understanding models, while incorporating inter-intra region connectivity across
              the visual cortex. Our neural encoding leverages the encoded dynamics from video
            stimuli, through utilizing two-stream networks and multiscale transformers, while
            taking connectivity priors into consideration. Our results show that merging both
            intra and inter-region connectivity priors increases the encoding performance over
            each one of them standalone or no connectivity priors. It also shows the necessity
            for encoding dynamics to fully benefit from such connectivity priors.</p>

          


        </div>
      </div>
    </div>
    <!--/ Abstract. -->


 
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      
      <!-- Fig2 -->
      <div class="column">
        <h2 class="title is-3">Artificial Neural Networks Target </h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/fig2.png" alt="Paper Image" class="interpolation-image">
            <p>
              Artificial neural network target experiments showing regression scores as Pearson’s correlation coefficient of image (blue) vs.
              video (red) model families on four target models; (a) MViT-B, (b) ViT-B, (c) ResNet-50, (d) I3D ResNet-50. We show the regression on
              the target network output features from their respective blocks, B1-7. 
              Statistical significance is shown at the bottom as ‘ns’ not significant,
            ‘∗, ∗∗, ∗ ∗ ∗’ significant with p-values < 0.05, 0.01, 0.001, respectively. It shows higher scores for the model family corresponding to the target
            network, especially in MViT and ViT.
                  </p>
            
          </div>

        </div>
      </div>
    </div>
    <!--/ fig2. -->
    <br><br>
    <!-- Cortical Regions Target -->
    <div class="column">
      <h2 class="title is-3">Cortical Regions Target</h2>
      <div class="columns is-centered">
        <div class="column content">
          <img src="./static/images/fig3.png" alt="Paper Image" class="interpolation-image">
          <p>
            Biological target experiments showing regression scores as Pearson’s correlation coefficient of model families on brain fMRI
            data. Comparison between: 
            (a) image vs. video understanding models, (b) convolutional vs. transformer-based models and (c) fully
          supervised vs. self-supervised models. 
            Statistical  significance is shown at the bottom as ‘ns’ not significant, ‘∗, ∗∗, ∗ ∗ ∗’ significant with
          p-values < 0.05, 0.01, 0.001, respectively. It shows video understanding models outperform single image ones, fully supervised outperform
          self-supervised ones and convolutional models surpass transformer-based ones in early-mid regions.
          </p>
          
        </div>

      </div>
    </div>
  <!--/ fig3. -->

  <br><br>
  <!-- Fig5 -->
  <div class="column">
    <h2 class="title is-3">Learned Directional Connectivity</h2>
    <div class="columns is-centered">
      <div class="column content">
        <img src="./static/images/fig5.png" alt="Paper Image" class="interpolation-image">
        <p>
          (a) Comparison of base model accuracies of MViT-B-16×4 and SlowFast and their accuracies after incorporating the intra-region
          and inter-region voxel connectivity showing the Pearson’s correlation coefficient as the regression scores. It shows the superiority of
          the connectivity-based models. </p>

      <p>
        (b) Comparison of performance enhancement by incorporating the intra-region and inter-region voxel
        connectivity together or each of them separately showing the Pearson’s correlation coefficient as the regression scores. It confirms the
        need for combining both intra- and inter-region connectivity. </p>

      <p>
        (c) Average weights per region contributing to the accuracy enhancement of
        each target visual region, showing the directional learned connectivity in our model. </p>
        
      </div>

    </div>

     <!-- Fig5 -->
     <br><br>
     <!-- Fig6 -->
  <div class="column">
    <h2 class="title is-3">Comparison to SOA Performance</h2>
    <div class="columns is-centered">
      <div class="column content">
        <img src="./static/images/fig6.png" alt="Paper Image" class="interpolation-image">
        <p>
          (a-c) Comparison between image and video models after incorporating connectivity priors. (a) ViT vs MViT, (b) ResNet50
          vs 3D ResNet50, and (c) ResNet50 vs SlowFast.</p> 

        <p>
          (d) Comparison between BMD SOA (using TSM ResNet50 third block features) and
        our models (MViT connectivity, TSM ResNet50 Base, and TSM ResNet50 Connectivity). Statistical significance is shown as ‘ns’ not
        significant, ‘∗, ∗∗, ∗ ∗ ∗’ significant with p-values < 0.05, 0.01, 0.001, respectively.</p>

        
      </div>

    </div>
</div>
<!--/ fig6. -->

</div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{gamal2024dynamics,
      title={Dynamics Based Neural Encoding with Inter-Intra Region Connectivity},
      author={Gamal, Mai and Rashad, Mohamed and Ehab, Eman and Eldawlatly, Seif and Siam, Mennatullah},
      journal={arXiv preprint arXiv:2402.12519},
      year={2024}
    }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2402.12519">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="coming-soon.html" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <h2>Acknowledgement:</h2>
          <p>
            This website is adapted from <a
            href="https://github.com/nerfies/nerfies.github.io?tab=readme-ov-file">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          
          
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
